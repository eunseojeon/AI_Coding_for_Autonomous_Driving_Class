{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgSgIjzxMggP2ilgSbCEmb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eunseojeon/AI_Coding_for_Autonomous_Driving_Class/blob/main/0715_CNN1_%ED%95%84%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sequential([\n",
        "  # 컨볼루션 블록 1\n",
        "  Conv2D(16, (3, 3), activation='relu', input_shape=(64,64,3)),\n",
        "  MaxPooling2D(2, 2),\n",
        "\n",
        "  # 컨볼루션 블록 2\n",
        "  Conv2D(32, (3,3), activation='relu'),\n",
        "  MaxPooling2D(2, 2),\n",
        "\n",
        "  # 컨볼루션 블록 3\n",
        "  Conv2D(64, (3, 3), activation='relu'),\n",
        "  MaxPooling2D(2, 2),\n",
        "\n",
        "  # 분류기\n",
        "  Flatten(),\n",
        "  Dense(128, activation= 'relu'),\n",
        "  Dropout(0.5),\n",
        "  Dense(3, activation='softmax') # Aninal/Car /Other\n",
        "  ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO7J7vbneI44",
        "outputId": "f19f2367-f3e4-49f9-e0dc-6720a8271530"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Sequential name=sequential_3, built=True>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLU(Rectified Linear Unit)\n",
        "- 딥러닝에서 가장 널리 사용되는 활성화 함수 중 하나입니다.\n",
        "- 이 함수는 입력값이 0보다 크면 그 값을 그대로 출력하고, 0 이하이면 0을 출력합니다.\n",
        "- 수식으로 표현하면 다음과 같습니다.\n",
        "- - **ReLU(x)=max(0,x)**\n",
        "- 즉, 입력값이 음수이면 0, 양수이면 입력값 자체가 출력됩니다.\n",
        "- ReLU의 주요 특징\n",
        "1. 비선형성: 선형 함수가 아니기 때문에, 딥러닝 모델에 비선형성을 추가할 수 있습니다.\n",
        "2. 계산 효율성: 연산이 매우 간단하여 학습 속도가 빠릅니다.\n",
        "3. 빠른 학습: 미분 가능한 영역이 넓어, 역전파 시 효율적으로 작동합니다.\n",
        "4. Dying ReLU 문제: 입력값이 계속 음수로만 들어오면 뉴런이 항상 0만 출력해서 학습이 멈추는 현상이 발생할 수 있습니다.\n",
        "- 파이썬에서 **ReLU** 구현 예시\n",
        "```\n",
        "import numpy as np\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "```\n",
        "- 이 함수는 넘파이 배열이나 스칼라 값 모두에 적용할 수 있습니다.\n",
        "- 그래프 예시: ReLU 함수의 그래프는 x축(입력)이 0 이하일 때 y=0, 0 초과일 때 y=x인 형태로, 0을 기준으로 꺾이는 직선입니다.\n"
      ],
      "metadata": {
        "id": "6XttEJG6fksc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. 입력층\n",
        "\n",
        "- **input_shape=(64,64,3)**:  \n",
        "  64x64 픽셀 크기의 RGB(3채널) 이미지를 입력으로 받습니다.\n",
        "\n",
        "#### 2. 컨볼루션 블록 (특징 추출 단계)\n",
        "\n",
        "각 블록은 Conv2D(합성곱)와 MaxPooling2D(최대 풀링)으로 구성되어 있습니다.\n",
        "\n",
        "| 블록 | Conv2D 필터 수 | 필터 크기 | 활성화 함수 | MaxPooling 크기 |\n",
        "|------|:-------------:|:---------:|:-----------:|:---------------:|\n",
        "| 1  | 16 | (3,3) | relu   | (2,2)   |\n",
        "| 2  | 32 | (3,3) | relu   | (2,2)   |\n",
        "| 3  | 64 | (3,3) | relu   | (2,2)   |\n",
        "\n",
        "- **Conv2D**:  \n",
        "  이미지에서 **특징(패턴)**을 추출합니다. 블록이 깊어질수록 더 복잡하고 추상적인 특징을 학습합니다.\n",
        "- **activation='relu'**:  \n",
        "  각 합성곱 결과에 **ReLU** 활성화 함수를 적용해 비선형성을 추가합니다.\n",
        "- **MaxPooling2D(2,2)**:  \n",
        "  특징 맵의 크기를 줄여 연산량을 감소시키고, 중요한 특징만 남깁니다.\n",
        "\n",
        "#### 3. 분류기 (Classification Head)\n",
        "\n",
        "- **Flatten()**:  \n",
        "  3차원 특징 맵을 1차원 벡터로 변환합니다(완전 연결층 입력을 위해).\n",
        "\n",
        "- **Dense(128, activation='relu')**:  \n",
        "  128개의 뉴런을 가진 완전 연결층. 이미지에서 추출된 특징을 바탕으로 학습합니다.\n",
        "\n",
        "- **Dropout(0.5)**:  \n",
        "  학습 시 50%의 뉴런을 랜덤하게 꺼서 **과적합(Overfitting)**을 방지합니다.\n",
        "\n",
        "- **Dense(3, activation='softmax')**:  \n",
        "  최종 출력층. 3개의 클래스(Animal, Car, Other)에 대해 **확률 분포**를 출력합니다.  \n",
        "  softmax 함수로 각 클래스가 정답일 확률을 계산합니다.\n",
        "\n",
        "### 전체 흐름 요약\n",
        "\n",
        "1. **입력 이미지** →  \n",
        "2. **합성곱 + 풀링(3회 반복)** →  \n",
        "3. **평탄화(Flatten)** →  \n",
        "4. **완전 연결층(Dense)** →  \n",
        "5. **드롭아웃(Dropout)** →  \n",
        "6. **출력층(Dense+Softmax)**  \n",
        "   → **3개 클래스 중 하나로 분류**\n"
      ],
      "metadata": {
        "id": "EA8AK3RxgUUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dense(128, activation='relu')와 Dense(3, activation='softmax') 설명\n",
        "\n",
        "이 두 줄은 딥러닝 모델의 **완전 연결층(fully connected layer, Dense layer)**을 의미하며, 이미지에서 추출된 특징을 바탕으로 최종 분류 작업을 수행합니다.\n",
        "\n",
        "**1. Dense(128, activation='relu')**\n",
        "\n",
        "- **의미**:  \n",
        "  128개의 뉴런(노드)로 구성된 완전 연결층입니다.\n",
        "- **역할**:  \n",
        "  앞 단계(Flatten 이후)에서 나온 특징 벡터를 받아, 더 높은 수준의 추상적 특징을 학습합니다.\n",
        "- **활성화 함수**:  \n",
        "  `activation='relu'`는 각 뉴런의 출력값에 ReLU(Rectified Linear Unit) 함수를 적용합니다.  \n",
        "  - 입력값이 0보다 크면 그대로, 0 이하이면 0을 출력하여 비선형성을 추가합니다.\n",
        "- **효과**:  \n",
        "  모델이 다양한 패턴과 복잡한 관계를 학습할 수 있게 도와줍니다.\n",
        "\n",
        "**2. Dense(3, activation='softmax') # Animal/Car/Other**\n",
        "\n",
        "- **의미**:  \n",
        "  3개의 뉴런으로 구성된 출력층입니다. 각 뉴런은 하나의 클래스(Animal, Car, Other)를 대표합니다.\n",
        "- **역할**:  \n",
        "  모델이 입력 이미지를 3가지 클래스 중 하나로 분류하도록 합니다.\n",
        "- **활성화 함수**:  \n",
        "  `activation='softmax'`는 각 뉴런의 출력값을 0~1 사이의 값으로 변환하고, 전체 합이 1이 되도록 만듭니다.  \n",
        "  - 각 값은 해당 클래스일 **확률**을 의미합니다.\n",
        "- **출력 해석**:  \n",
        "  예를 들어 `[0.7, 0.2, 0.1]`이 출력되었다면, 첫 번째 클래스(Animal)일 확률이 70%라는 뜻입니다.\n",
        "- **최종 예측**:  \n",
        "  가장 높은 확률을 가진 클래스를 선택하여 예측 결과로 사용합니다.\n",
        "\n",
        "#### 요약 표\n",
        "\n",
        "| 층 (Layer)| 뉴런 수 | 활성화 함수 | 역할/설명 |\n",
        "|---------------------------|---------|-------------|-----------------------------------------------------------------|\n",
        "| Dense(128, activation='relu')| 128     | ReLU        | 특징 벡터를 받아 고차원 특징 추출, 비선형성 부여                |\n",
        "| Dense(3, activation='softmax')  | 3       | Softmax     | 3개 클래스(Animal/Car/Other) 확률 출력, 최종 분류 결정           |\n",
        "\n",
        "이 두 층은 모델이 이미지의 복잡한 특징을 바탕으로 **정확하게 분류**할 수 있도록 핵심적인 역할을 합니다."
      ],
      "metadata": {
        "id": "MQ8XBT3NhYH3"
      }
    }
  ]
}