# ✅ 전이학습이란?
- **전이학습**(Transfer Learning)은 한 문제(domain)에서 학습한 모델의 지식이나 파라미터를 다른 문제로 전이하여 사용하는 머신러닝 기법입니다.
- 기존의 딥러닝은 대규모 데이터와 연산을 필요로 하지만, 전이학습을 이용하면 **적은 데이터로도 효과적으로 모델 성능을 높일** 수 있습니다.

### 1️⃣ 전이학습의 주요 개념

- **소스 도메인(Source Domain)**: 기존에 학습이 이루어졌던 데이터와 과제
- **타깃 도메인(Target Domain)**: 지식을 이전받아 적용할 새로운 데이터와 과제
- **파인튜닝(Fine-tuning)**: 기존 모델의 일부 혹은 전체 파라미터를 새로운 데이터에 맞게 조금씩 조정하는 과정
- **Feature Extraction**: 사전 학습된 모델에서 특징만 추출하여 새로운 문제에 사용

### 2️⃣ 전이학습의 방식

| 방식            | 설명                                                                      |
|-----------------|---------------------------------------------------------------------------|
| Feature-based   | 기존 모델의 feature extractor만 사용, 마지막 분류기 부분만 새로 학습          |
| Fine-tuning     | 전체 또는 일부 레이어를 타깃 도메인 데이터로 추가 학습                      |
| Multi-task      | 여러 작업을 동시에 학습하며, 공유된 지식을 서로 다른 작업에 활용               |
| Zero-shot/One-shot | 소량 또는 전혀 없는 레이블 데이터로 타깃 도메인을 처리                     |

### 3️⃣ 전이학습의 대표적 활용 예시

- **컴퓨터 비전**: ImageNet 등 대규모 이미지 데이터로 사전 학습된 CNN 모델을, 도메인 특화 데이터(예: 의료 이미지)에 파인튜닝
- **자연어 처리**: BERT, GPT 등 대규모 텍스트 데이터에서 사전 학습된 언어모델을, 질의응답 또는 감정분석 등 특정 작업에 적용
- **음성 인식**: 대규모 음성 데이터로 학습된 모델을 개별 사용자 음성 인식에 적용

### 4️⃣ 전이학습의 장점과 한계

- **장점**
  - 적은 데이터로도 좋은 성능
  - 학습 시간 및 리소스 절감
  - 소규모 데이터셋에 강점

- **한계**
  - 소스와 타깃 도메인 사이 차이가 크면 효과↓
  - 비정상적인 전이(negative transfer) 가능성
  - 하이퍼파라미터 튜닝의 어려움

### 5️⃣ 전이학습의 대표 모델

- **비전**: ResNet, VGG, Inception 등
- **자연어 처리**: BERT, GPT, RoBERTa, T5 등

### 6️⃣ 전이학습의 실습 예시 (PyTorch 기준)

```
python
import torch
import torchvision.models as models

# 사전 학습된 모델 로드
model = models.resnet50(pretrained=True)

# 마지막 분류 레이어 교체 (예: 클래스 2개인 경우)
import torch.nn as nn
model.fc = nn.Linear(model.fc.in_features, 2)

# 파인튜닝
for param in model.parameters():
    param.requires_grad = False  # feature extraction
for param in model.fc.parameters():
    param.requires_grad = True

# 이후 학습 루프 및 옵티마이저 설정
```

### 7️⃣ 대표 프레임워크 & 실전 코드

- **PyTorch**
```
python
import torch
import torchvision.models as models
import torch.nn as nn

model = models.resnet50(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, 2)  # 분류기 교체

for param in model.parameters():
    param.requires_grad = False  # Feature extraction
for param in model.fc.parameters():
    param.requires_grad = True   # Classifier만 학습
```

- **TensorFlow/Keras**
  
```
python
from tensorflow.keras.applications import VGG16
from tensorflow.keras import layers, models

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))
base_model.trainable = False  # Feature extractor만 사용

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(2, activation='softmax')
])
```

### 8️⃣ 전이학습의 최신 트렌드 및 고급 기법

- **도메인 적응(Domain Adaptation):** 데이터 분포가 다른 소스와 타깃 사이 격차를 줄이는 기술
- **멀티태스크 러닝(Multi-task Learning):** 여러 작업에서 공통 특성 추출해 일반화 능력 강화
- **제로샷/퓨샷 러닝:** 데이터 부족 상황에서의 일반화(대규모 언어 모델 등)
- **연속적 학습(Lifelong/Continual Learning):** 여러 작업을 순차적으로 학습하면서 과거 경험을 보존하는 연구
- **페더레이티드 러닝:** 여러 데이터 소스를 분산형으로 학습, 데이터 프라이버시와 안전 고려


### 9️⃣ 전이학습 산업 응용사례

- **의료:** 암 진단, 병변 탐지 등 의료 이미지 분석
- **금융:** 신용평가, 사기 탐지
- **제조·산업:** 불량 감지, 설비 예측 유지보수
- **자율주행:** 차량 탑재 센서에서 수집된 데이터별로 특화된 학습

### 🔟 참고자료 (추천 링크 및 논문)

- "Transfer Learning — Deep Learning Glossary", DeepAI.
- "A comprehensive introduction to transfer learning", Towards Data Science.
- "Transfer Learning in NLP", Google Developers.
- Papers with Code: Transfer Learning 분야 최신 코드·논문
- "A Survey on Transfer Learning": 전이학습 최신 동향/리뷰논문



